{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fbd232",
   "metadata": {},
   "source": [
    "## Numpy array reduce example\n",
    "\n",
    "Let each MPI process create a 10-elements numpy array, initialized with its own rank number.\n",
    "Let process 0 calculate the total sum of all numpy arrays element-wise.\n",
    "You can use the hints and template given below.\n",
    "\n",
    "```\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank =                 # get process rank\n",
    "size =                 # get total number of processes\n",
    "\n",
    "sendbuf = np.zeros(10, dtype='i') + rank\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "  recvbuf = np.zeros(10, dtype='i')\n",
    "comm.Reduce(, , op= , root=0)    # What should be reduced? And which operation is used?\n",
    "\n",
    "if rank == 0:\n",
    "  sum = sum(range(size))\n",
    "  assert (recvbuf[:]==sum).all()\n",
    "  print(recvbuf)\n",
    "```\n",
    "\n",
    "The result of the exercise should look like:\n",
    "\n",
    "```\n",
    "$mpirun -np 4 python3 reducenumpy.py \n",
    "[6 6 6 6 6 6 6 6 6 6]\n",
    "```\n",
    "\n",
    "```\n",
    "$mpirun -np 5 python3 reducenumpy.py \n",
    "[10 10 10 10 10 10 10 10 10 10]\n",
    "```\n",
    "\n",
    "**Extra:** can you make Rank = 1 print the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fae5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0050b6b",
   "metadata": {},
   "source": [
    "# MPI Matrix Multiplication example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "533b2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpi_matmul.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_matmul.py\n",
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import math\n",
    " \n",
    "def explicit_matmul(A,B):\n",
    "    #A[m][n]\n",
    "    #B[n][p]\n",
    "    #C[m][p]   \n",
    "    C_temp = [[0 for x in range(np.shape(A)[0])] for y in range(np.shape(B)[1])]\n",
    "    for i in range(np.shape(A)[0]): #(i=1...m) Rows in A\n",
    "        for j in range(np.shape(B)[1]): # (j=1...p) Columns in B\n",
    "            for k in range(np.shape(A)[1]): # (k=1...n) Columns in A\n",
    "                C_temp[i][j] += A[i][k] * B[k][j]\n",
    "    return(C_temp)\n",
    "  \n",
    "def explicit_matmul_mpi(A,B,rank,size):\n",
    "    #A[m][n]\n",
    "    #B[n][p]\n",
    "    #C[m][p]\n",
    "    C_temp = [[0 for x in range(np.shape(A)[0])] for y in range(np.shape(B)[1])]\n",
    "    for i in range(np.shape(A)[0]): #(i=1...m) Rows in A\n",
    "        for j in range(np.shape(B)[1]): # (j=1...p) Columns in B\n",
    "            step = math.floor(np.shape(A)[1]/size)\n",
    "            if (np.shape(A)[1]/size % 1 != 0) and (rank == size-1):\n",
    "                for k in range(rank*step,np.shape(A)[1]):\n",
    "                    C_temp[i][j] += A[i][k] * B[k][j]\n",
    "            else:\n",
    "                for k in range(rank*step,rank*step+step):\n",
    "                    C_temp[i][j] += A[i][k] * B[k][j]\n",
    "             \n",
    "    return(C_temp)\n",
    " \n",
    "\n",
    "t1_mpi = MPI.Wtime()\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "\n",
    "#1: initiate the communicator\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "#2: Initialize the arrays A,B... Since we are using np.random, they will not be the same per rank!!!\n",
    "if comm.Get_rank() == 0:\n",
    "    AX=AY=BX=BY=100\n",
    "    A = np.random.rand(AX,AY)\n",
    "    B = np.random.rand(BX,BY)\n",
    "else:\n",
    "    A = None\n",
    "    B = None\n",
    "    \n",
    "#3: Broadcast them to the other ranks!\n",
    "A = comm.bcast(A,root=0)\n",
    "B = comm.bcast(B,root=0)\n",
    " \n",
    "if comm.Get_rank() == 0:\n",
    "    print(\"============================================================================\")\n",
    "    print(\"Performing Matrix Multiplication of two matricies of size (%d,%d)\" % (AX,AY) )\n",
    "    print(\"Using %d parallel MPI processes\" % comm.Get_size())\n",
    "     \n",
    "#4: calc matrix multiply for each rank\n",
    "result = explicit_matmul_mpi(A,B,comm.Get_rank(),comm.Get_size())\n",
    " \n",
    "#5: \"Gather\" the results from the ranks and Make sure results are put back together and correctly!!!\n",
    "### HINT: use np.sum! \n",
    "C_mpi_parallel = comm.gather(result)\n",
    "C_mpi_parallel = np.sum(C_mpi_parallel,axis=0)\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    " \n",
    "t2_mpi = MPI.Wtime()\n",
    " \n",
    "if comm.Get_rank() == 0:\n",
    "    print(\"============================================================================\")\n",
    "        \n",
    "    t1_explicit = MPI.Wtime()\n",
    "    C_explicit = explicit_matmul(A,B)\n",
    "    t2_explicit = MPI.Wtime()\n",
    " \n",
    "    t1_numpy = MPI.Wtime()\n",
    "    C_np = np.matmul(A,B)\n",
    "    t2_numpy = MPI.Wtime()\n",
    " \n",
    "    if not np.allclose(C_explicit, C_mpi_parallel, rtol=1e-10, atol=1e-10):\n",
    "        print(\"C_parallel is not equal to C_explicit!!\")\n",
    "    if not np.allclose(C_mpi_parallel, C_np, rtol=1e-10, atol=1e-10):\n",
    "        print(\"C_parallel is not equal to C_np!!\")\n",
    "    if not np.allclose(C_explicit, C_np, rtol=1e-10, atol=1e-10):\n",
    "        print(\"C_np is not equal to C_explicit!!\")\n",
    "     \n",
    "    print(\"Performance=======\")\n",
    "    print(\"explicit: \",t2_explicit-t1_explicit)\n",
    "    print(\"MPI: \",t2_mpi-t1_mpi)\n",
    "    print(\"numpy matmul: \",t2_numpy-t1_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7505aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Performing Matrix Multiplication of two matricies of size (100,100)\n",
      "Using 3 parallel MPI processes\n",
      "============================================================================\n",
      "Performance=======\n",
      "explicit:  0.5971695429999999\n",
      "MPI:  0.269423894\n",
      "numpy matmul:  0.0012420200000000658\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 3 --oversubscribe python3 mpi_matmul.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e43ceb",
   "metadata": {},
   "source": [
    "### submit it to the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10346a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpi_jobscript.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_jobscript.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -n 32\n",
    "#SBATCH -p normal\n",
    "#SBATCH -t 00:30:00\n",
    "\n",
    "module load 2021\n",
    "module load SciPy-bundle/2021.05-foss-2021a\n",
    "\n",
    "mpirun -np 32 --oversubscribe python3 mpi_matmult.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5d28ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10311904\r\n"
     ]
    }
   ],
   "source": [
    "!sbatch mpi_jobscript.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b73208",
   "metadata": {},
   "source": [
    "## Try to improve the example above: Communication of buffer-like objects\n",
    "\n",
    "**hints**: You need to \"flatten\" or \"ravel\" 2D numpy arrays to 1D numpy arrays in order to communicate the Numpy arrays \n",
    "- Go from 2D -> 1D `A = np.random.randint(10, size=(AX, AY))` and `A_buffer =  A.ravel()`\n",
    "- Go from 1D -> 2D `A = np.reshape(A_buffer, (AX,AY))`\n",
    "- A reduce can also help out here `comm.Reduce(result_sendbuffer, C_buffer, root=0)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
