{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6182eb6f",
   "metadata": {},
   "source": [
    "# Parallel programming for CPU\n",
    "\n",
    "## process based parallelism via the concurrent.futures module \n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e549a3f",
   "metadata": {},
   "source": [
    "## ProcessPoolExecutor (concurrent.futures)\n",
    "\n",
    "The ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the **Global Interpreter Lock** but also means that only picklable objects can be executed and returned. Each Process is a true system process without shared memory. If shared memory is needed the multiprocessing module provides features for sharing data and passing messages between them so that in many cases converting from threads to processes is as simple as changing a few import statements.\n",
    "- “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. Examples are `None`, `True`, and `False` ... integers, floating-point numbers, complex numbers ... strings, bytes, bytearrays ... tuples, lists, sets, and dictionaries containing only picklable objects.... and many more .... https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled\n",
    "\n",
    "**NOTE** An important difference with processes vs threads is that each child process needd to import the script containing the target function. Therefore it is important to wrap the main part of the application with `__main__` to ensure this part is not executed by every child process. Alternatively the target function can be stored in a different file that can be then imported into the main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba93a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process_id.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_id.py\n",
    "from concurrent import futures\n",
    "import os\n",
    "import time\n",
    "\n",
    "def do_work(n):\n",
    "    time.sleep(n)\n",
    "    return (n, os.getpid())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tasks = range(1,5)\n",
    "    ex = futures.ProcessPoolExecutor(max_workers=len(tasks))\n",
    "    results = ex.map(do_work, tasks)\n",
    "    for n, pid in results:\n",
    "        print('ran task {} in process {}'.format(n, pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d056ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran task 1 in process 3952291\n",
      "ran task 2 in process 3952294\n",
      "ran task 3 in process 3952295\n",
      "ran task 4 in process 3952296\n"
     ]
    }
   ],
   "source": [
    "!python process_id.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2adb9",
   "metadata": {},
   "source": [
    "Just to check.... What are the PID's that area assigned to each thread using the ThreadPoolExecutor class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02a542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran task 1 in process 3951951\n",
      "ran task 2 in process 3951951\n",
      "ran task 3 in process 3951951\n",
      "ran task 4 in process 3951951\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "import os\n",
    "import time\n",
    "\n",
    "def do_work(n):\n",
    "    time.sleep(n)\n",
    "    return (n, os.getpid())\n",
    "\n",
    "tasks = range(1,5)\n",
    "ex = futures.ThreadPoolExecutor(max_workers=len(tasks))\n",
    "results = ex.map(do_work, tasks)\n",
    "for n, pid in results:\n",
    "    print('ran task {} in process {}'.format(n, pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26113f7d-4769-4082-bd90-276fe75afdd7",
   "metadata": {},
   "source": [
    "## Compute Bound code (threads vs Processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784516d6",
   "metadata": {},
   "source": [
    "### Example of CPU bound code (Threads vs Processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6e703810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prime.py\n",
    "from concurrent import futures\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    sqrt_n = int(math.floor(math.sqrt(n)))\n",
    "    for i in range(3, sqrt_n + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def processes(PRIMES,num_workers):\n",
    "    \n",
    "    ex = futures.ProcessPoolExecutor(max_workers=num_workers)\n",
    "    fut = ex.map(is_prime, PRIMES)\n",
    "    \n",
    "    return(list(fut))\n",
    "\n",
    "def threads(PRIMES,num_workers):\n",
    "    \n",
    "    ex = futures.ThreadPoolExecutor(max_workers=num_workers)\n",
    "    fut = ex.map(is_prime, PRIMES)\n",
    "    \n",
    "    return(list(fut))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    PRIMES = [0, 2, 4, 5, 7, 10]\n",
    "\n",
    "    # make it more intesive\n",
    "    list_size = 1000\n",
    "    PRIMES = [random.randrange(100000000000000, 500000000000000, 1) for i in range(list_size)]\n",
    "\n",
    "    p_start = time.time()\n",
    "    result_p = processes(PRIMES,16)\n",
    "    p_end = time.time()\n",
    "    t_start = time.time()\n",
    "    result_t = threads(PRIMES,16)\n",
    "    t_end = time.time()\n",
    "    \n",
    "    print(\"\\n\\nTimings:\")\n",
    "    print(\"Processes: {}\".format(p_end - p_start))    \n",
    "    print(\"Threads  : {}\".format(t_end - t_start))\n",
    "    \n",
    "    #print(result_p)\n",
    "    #print(\"N_prime \", result_p.count(True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d90cd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Timings:\n",
      "Processes: 2.0788538455963135\n",
      "Threads  : 25.077348947525024\n"
     ]
    }
   ],
   "source": [
    "!python prime.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99a02e",
   "metadata": {},
   "source": [
    "### Further Process control via the multiprocessing module \n",
    "\n",
    "The [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - module mirrors threading, except that instead of a Thread class it provides a Process. Each Process is a true system process without shared memory, but multiprocessing provides features for sharing data and passing messages between them so that in many cases converting from threads to processes is as simple as changing a few import statements.\n",
    "\n",
    "multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the `Global Interpreter Lock` by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.\n",
    "\n",
    "The multiprocessing module also introduces APIs which do not have analogs in the threading module. A prime example of this is the `Pool` object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism). The following example demonstrates the common practice of defining such functions in a module so that child processes can successfully import that module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
